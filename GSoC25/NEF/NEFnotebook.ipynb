{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omeu81kHRmr6",
        "outputId": "200947f0-73d4-44bc-8132-5b79333361ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting redis\n",
            "  Downloading redis-6.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
            "  Downloading rdflib-7.2.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Downloading redis-6.4.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading rdflib-7.2.1-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.4/565.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=b71a79595269fbe800fad3e67dc8157de36ecf2b536b1af83272bbd9a1c6bd00\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: redis, rdflib, wikipedia, SPARQLWrapper\n",
            "Successfully installed SPARQLWrapper-2.0.0 rdflib-7.2.1 redis-6.4.0 wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai pandas redis wikipedia nltk SPARQLWrapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Add the cloned repo to Python path\n",
        "os.environ['GEMINI_API_KEY'] = 'AIzaSyBa0V945z1PfYlv71h8L0JTXEe8CnakuVY'"
      ],
      "metadata": {
        "id": "uLjvswkzSJSq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Colab setup ===\n",
        "!pip -q install rdflib\n",
        "\n",
        "import os, math, time, sys, json, requests, re, unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdflib import Graph, Namespace\n",
        "\n",
        "# === API key (Gemini Developer API) ===\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\").strip()\n",
        "if not API_KEY:\n",
        "    from getpass import getpass\n",
        "    API_KEY = getpass(\"Enter your Gemini API key: \").strip()\n",
        "\n",
        "# == Gemini Embeddings REST (batch) ==\n",
        "BATCH_ENDPOINT = \"https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents\"\n",
        "HEADERS = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "# ==== Helpers ====\n",
        "def _localname(uri: str) -> str:\n",
        "    if \"#\" in uri:\n",
        "        return uri.split(\"#\")[-1]\n",
        "    return uri.rstrip(\"/\").split(\"/\")[-1]\n",
        "\n",
        "def _split_camel(name: str) -> str:\n",
        "    s = re.sub(r\"(?<=[a-z0-9])([A-Z])\", r\" \\1\", name)\n",
        "    s = s.replace(\"_\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    return s\n",
        "\n",
        "def make_label_text(uri: str, rdfs_label: str | None, rdfs_comment: str | None) -> str:\n",
        "    \"\"\"\n",
        "    Build a neutral, automatic label string for embedding.\n",
        "    - mechanical localname split (no hardcoded meanings)\n",
        "    - include rdfs:label / rdfs:comment from the ontology when available\n",
        "    \"\"\"\n",
        "    ln = _localname(uri)\n",
        "    ln_clean = _split_camel(ln)\n",
        "    parts = [f\"predicate: {ln_clean}\"]\n",
        "    if rdfs_label:\n",
        "        parts.append(f\"label: {rdfs_label}\")\n",
        "    if rdfs_comment:\n",
        "        parts.append(f\"comment: {rdfs_comment}\")\n",
        "    return \". \".join(parts)\n",
        "\n",
        "# === Load DBpedia ontology ===\n",
        "g = Graph()\n",
        "# dbpedia ontology (public URL)\n",
        "g.parse(\"http://dief.tools.dbpedia.org/server/ontology/dbpedia.owl\")\n",
        "print(f\"Total triples in graph: {len(g)}\")\n",
        "\n",
        "# --- SPARQL: collect predicates with english labels/comments ---\n",
        "query = \"\"\"\n",
        "SELECT ?p ?label ?comment WHERE {\n",
        "  ?p a rdf:Property ;\n",
        "     rdfs:label ?label .\n",
        "  FILTER(LANG(?label) = \"en\")\n",
        "  OPTIONAL {\n",
        "    ?p rdfs:comment ?comment .\n",
        "    FILTER(LANG(?comment) = \"en\")\n",
        "  }\n",
        "}\n",
        "ORDER BY ?p\n",
        "\"\"\"\n",
        "\n",
        "df = pd.DataFrame(g.query(query), columns=[\"predicate\", \"label\", \"comment\"])\n",
        "print(f\"Total predicates (rows): {len(df)}\")\n",
        "print(df.head(5))\n",
        "\n",
        "# Extract unique predicate rows by URI (keep first label/comment seen)\n",
        "df[\"predicate\"] = df[\"predicate\"].astype(str)\n",
        "uniq = df.drop_duplicates(subset=[\"predicate\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "predicates = uniq[\"predicate\"].tolist()\n",
        "labels = uniq[\"label\"].astype(str).tolist()\n",
        "comments = uniq[\"comment\"].apply(lambda x: str(x) if not pd.isna(x) else \"\").tolist()\n",
        "\n",
        "print(f\"Unique predicate URIs: {len(predicates)}\")\n",
        "\n",
        "# === Build texts to embed (semantic, not raw URIs) ===\n",
        "texts = [make_label_text(u, l, c if c else None) for u, l, c in zip(predicates, labels, comments)]\n",
        "\n",
        "# === Embedding config ===\n",
        "# Force output dimensionality so it matches your runtime (e.g., 768)\n",
        "OUTPUT_DIM = 768\n",
        "BATCH_SIZE = 100\n",
        "MAX_RETRIES = 5\n",
        "SLEEP_BETWEEN_CALLS = 0.4  # pacing for free-tier rate limits\n",
        "\n",
        "def make_requests_payload(text_batch):\n",
        "    \"\"\"\n",
        "    Each item in 'requests' is one EmbedContentRequest.\n",
        "    We pass outputDimensionality PER REQUEST to guarantee the shape.\n",
        "    \"\"\"\n",
        "    requests_payload = []\n",
        "    for text in text_batch:\n",
        "        requests_payload.append({\n",
        "            \"model\": \"models/embedding-001\",\n",
        "            \"content\": {\"parts\": [{\"text\": text}]},\n",
        "            \"outputDimensionality\": OUTPUT_DIM,\n",
        "            # Optional: \"taskType\": \"RETRIEVAL_DOCUMENT\",\n",
        "        })\n",
        "    return {\"requests\": requests_payload}\n",
        "\n",
        "def embed_batch_rest(text_batch):\n",
        "    # Simple retry with backoff\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            payload = make_requests_payload(text_batch)\n",
        "            resp = requests.post(\n",
        "                f\"{BATCH_ENDPOINT}?key={API_KEY}\",\n",
        "                headers=HEADERS,\n",
        "                data=json.dumps(payload),\n",
        "                timeout=90,\n",
        "            )\n",
        "            if resp.status_code == 200:\n",
        "                data = resp.json()\n",
        "                # Expected: {\"embeddings\": [{\"values\":[...]} , ... ]}\n",
        "                if \"embeddings\" not in data or not isinstance(data[\"embeddings\"], list):\n",
        "                    raise RuntimeError(f\"Unexpected response format: {data}\")\n",
        "                return [e[\"values\"] for e in data[\"embeddings\"]]\n",
        "            else:\n",
        "                try:\n",
        "                    err = resp.json()\n",
        "                except Exception:\n",
        "                    err = resp.text\n",
        "                raise RuntimeError(f\"HTTP {resp.status_code}: {err}\")\n",
        "        except Exception as e:\n",
        "            wait = min(2 ** attempt, 30)\n",
        "            print(f\"[embed_batch_rest] attempt {attempt} failed: {e}\\nSleeping {wait}s...\", file=sys.stderr)\n",
        "            time.sleep(wait)\n",
        "    raise RuntimeError(\"Failed to embed after retries.\")\n",
        "\n",
        "# === Run embeddings in batches ===\n",
        "all_vecs = []\n",
        "n = len(texts)\n",
        "num_batches = math.ceil(n / BATCH_SIZE)\n",
        "print(f\"Embedding {n} predicates (dim={OUTPUT_DIM}) in {num_batches} batch(es), size {BATCH_SIZE}...\")\n",
        "\n",
        "for b in range(num_batches):\n",
        "    s = b * BATCH_SIZE\n",
        "    e = min((b + 1) * BATCH_SIZE, n)\n",
        "    batch_texts = texts[s:e]\n",
        "    vecs = embed_batch_rest(batch_texts)\n",
        "    if len(vecs) != len(batch_texts):\n",
        "        raise RuntimeError(f\"Vector count mismatch in batch {b+1}: got {len(vecs)} for {len(batch_texts)} texts\")\n",
        "    all_vecs.extend(vecs)\n",
        "    print(f\"  Batch {b+1}/{num_batches}: {len(batch_texts)} items → {len(vecs)} vectors\")\n",
        "    time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "\n",
        "# === Finalize\n",
        "embeddings = np.asarray(all_vecs, dtype=np.float32)\n",
        "print(\"\\n✅ Done!\")\n",
        "print(f\"Total vectors: {embeddings.shape[0]}\")\n",
        "print(f\"Vector dim: {embeddings.shape[1] if embeddings.size else 'N/A'}\")\n",
        "print(\"Example label:\", texts[0])\n",
        "print(\"First vector (first 10 dims):\", embeddings[0][:10].tolist() if embeddings.size else \"N/A\")\n",
        "\n",
        "# === Save outputs ===\n",
        "# We keep 'predicates.csv' as the ID list (URIs),\n",
        "# and create a companion 'predicate_labels.csv' for transparency/debug.\n",
        "pred_out = \"predicates.csv\"\n",
        "labels_out = \"predicate_labels.csv\"\n",
        "vec_out = \"embeddings.npy\"\n",
        "\n",
        "pd.DataFrame({\"predicate\": predicates}).to_csv(pred_out, index=False)\n",
        "pd.DataFrame({\"predicate\": predicates, \"label_text\": texts}).to_csv(labels_out, index=False)\n",
        "np.save(vec_out, embeddings)\n",
        "\n",
        "print(f\"\\nSaved:\")\n",
        "print(f\"- {pred_out} (URIs)\")\n",
        "print(f\"- {labels_out} (what was embedded)\")\n",
        "print(f\"- {vec_out} (shape: {embeddings.shape})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXPKgKKwSM_d",
        "outputId": "017780a7-c702-4359-ce68-af06fa6c8656"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total triples in graph: 34544\n",
            "Total predicates (rows): 2884\n",
            "                                           predicate  \\\n",
            "0                  http://dbpedia.org/ontology/aSide   \n",
            "1    http://dbpedia.org/ontology/abbeychurchBlessing   \n",
            "2  http://dbpedia.org/ontology/abbeychurchBlessin...   \n",
            "3           http://dbpedia.org/ontology/abbreviation   \n",
            "4            http://dbpedia.org/ontology/ableToGrind   \n",
            "\n",
            "                          label comment  \n",
            "0                        a side    None  \n",
            "1         abbey church blessing    None  \n",
            "2  abbey church blessing charge    None  \n",
            "3                  abbreviation    None  \n",
            "4                 able to grind    None  \n",
            "Unique predicate URIs: 2884\n",
            "Embedding 2884 predicates (dim=768) in 29 batch(es), size 100...\n",
            "  Batch 1/29: 100 items → 100 vectors\n",
            "  Batch 2/29: 100 items → 100 vectors\n",
            "  Batch 3/29: 100 items → 100 vectors\n",
            "  Batch 4/29: 100 items → 100 vectors\n",
            "  Batch 5/29: 100 items → 100 vectors\n",
            "  Batch 6/29: 100 items → 100 vectors\n",
            "  Batch 7/29: 100 items → 100 vectors\n",
            "  Batch 8/29: 100 items → 100 vectors\n",
            "  Batch 9/29: 100 items → 100 vectors\n",
            "  Batch 10/29: 100 items → 100 vectors\n",
            "  Batch 11/29: 100 items → 100 vectors\n",
            "  Batch 12/29: 100 items → 100 vectors\n",
            "  Batch 13/29: 100 items → 100 vectors\n",
            "  Batch 14/29: 100 items → 100 vectors\n",
            "  Batch 15/29: 100 items → 100 vectors\n",
            "  Batch 16/29: 100 items → 100 vectors\n",
            "  Batch 17/29: 100 items → 100 vectors\n",
            "  Batch 18/29: 100 items → 100 vectors\n",
            "  Batch 19/29: 100 items → 100 vectors\n",
            "  Batch 20/29: 100 items → 100 vectors\n",
            "  Batch 21/29: 100 items → 100 vectors\n",
            "  Batch 22/29: 100 items → 100 vectors\n",
            "  Batch 23/29: 100 items → 100 vectors\n",
            "  Batch 24/29: 100 items → 100 vectors\n",
            "  Batch 25/29: 100 items → 100 vectors\n",
            "  Batch 26/29: 100 items → 100 vectors\n",
            "  Batch 27/29: 100 items → 100 vectors\n",
            "  Batch 28/29: 100 items → 100 vectors\n",
            "  Batch 29/29: 84 items → 84 vectors\n",
            "\n",
            "✅ Done!\n",
            "Total vectors: 2884\n",
            "Vector dim: 768\n",
            "Example label: predicate: a Side. label: a side\n",
            "First vector (first 10 dims): [0.006285395938903093, -0.054129354655742645, 0.014049907214939594, -0.004998000804334879, 0.013980191200971603, 0.042770981788635254, 0.03229377791285515, -0.01388034038245678, 0.013374832458794117, 0.020869821310043335]\n",
            "\n",
            "Saved:\n",
            "- predicates.csv (URIs)\n",
            "- predicate_labels.csv (what was embedded)\n",
            "- embeddings.npy (shape: (2884, 768))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U google-genai google-generativeai redis pandas numpy requests\n"
      ],
      "metadata": {
        "id": "lL8tW1dlTBLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bc0004-5024-4315-ff34-1ba96eceabd2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.5/238.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Minimal NEF pipeline (lean) ---\n",
        "# Keeps: Redis entity linking + precomputed predicate retrieval + LLM disambiguation.\n",
        "# Drops: pandas dependency, unused helpers, excess prints. Adds small fixes (quote import, tighter JSON parse).\n",
        "\n",
        "import os, json, re, time, sys\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Sequence, Dict, Any, Optional\n",
        "from urllib.parse import quote\n",
        "\n",
        "from getpass import getpass\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- API key bootstrap ---\n",
        "if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "    os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter your Google (Gemini) API key: \").strip()\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# =============== Utils ===============\n",
        "\n",
        "def _normalize(vec: Sequence[float]) -> np.ndarray:\n",
        "    v = np.asarray(vec, dtype=np.float32)\n",
        "    n = float(np.linalg.norm(v)) or 1e-12\n",
        "    return v / n\n",
        "\n",
        "def _json_from_model(text: str) -> Any:\n",
        "    \"\"\"Extract the first JSON object/array from a model string.\"\"\"\n",
        "    t = (text or \"\").strip()\n",
        "    t = re.sub(r\"^```(?:json)?|```$\", \"\", t, flags=re.IGNORECASE | re.MULTILINE).strip()\n",
        "    m = re.search(r\"\\{.*\\}|\\[.*\\]\", t, flags=re.DOTALL)\n",
        "    if not m:\n",
        "        raise ValueError(\"No JSON object/array found in model output.\")\n",
        "    return json.loads(m.group(0))\n",
        "\n",
        "def _safe_print(*args, **kwargs):\n",
        "    try:\n",
        "        print(*args, **kwargs)\n",
        "    except Exception:\n",
        "        # Avoid crashing on weird unicode in some environments\n",
        "        sys.stdout.write((\" \".join(map(str, args)) + \"\\n\").encode(\"utf-8\", \"ignore\").decode(\"utf-8\"))\n",
        "\n",
        "# =============== Redis Entity Linking ===============\n",
        "\n",
        "class RedisEntityLinking:\n",
        "    \"\"\"Redis-based entity linking; degrades gracefully if Redis is unavailable.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        host: str = os.getenv(\"NEF_REDIS_HOST\", \"91.99.92.217\"),\n",
        "        port: int = int(os.getenv(\"NEF_REDIS_PORT\", \"6379\")),\n",
        "        password: str = os.getenv(\"NEF_REDIS_PASSWORD\", \"NEF!gsoc2025\"),\n",
        "        connect_timeout: float = 2.0,\n",
        "    ):\n",
        "        self.available = False\n",
        "        self.redis_forms = None\n",
        "        self.redis_redir = None\n",
        "        try:\n",
        "            import redis  # local import so code runs even if redis isn't installed\n",
        "            common = dict(host=host, port=port, password=password, socket_connect_timeout=connect_timeout,\n",
        "                          decode_responses=True)\n",
        "            self.redis_forms = redis.Redis(db=0, **common)\n",
        "            self.redis_redir  = redis.Redis(db=1, **common)\n",
        "            self.available = bool(self.redis_forms.ping() and self.redis_redir.ping())\n",
        "            _safe_print(\"✓ Connected to Redis\" if self.available else \"✗ Redis ping failed\")\n",
        "        except Exception as e:\n",
        "            _safe_print(f\"✗ Redis connection error (continuing with fallbacks): {e}\")\n",
        "\n",
        "    def _redirect(self, uri: str, max_hops: int = 10) -> str:\n",
        "        if not self.available:\n",
        "            return uri\n",
        "        seen = set()\n",
        "        cur = uri\n",
        "        for _ in range(max_hops):\n",
        "            if cur in seen:\n",
        "                break\n",
        "            seen.add(cur)\n",
        "            nxt = self.redis_redir.get(cur)\n",
        "            if not nxt:\n",
        "                return cur\n",
        "            cur = nxt\n",
        "        return cur\n",
        "\n",
        "    def lookup(self, surface_form: str, top_k: int = 5, thr: float = 0.01) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Returns list of (entity_uri, score). Score is normalized by max support.\n",
        "        \"\"\"\n",
        "        if not self.available or not surface_form.strip():\n",
        "            return []\n",
        "        raw = self.redis_forms.hgetall(surface_form)\n",
        "        if not raw:\n",
        "            return []\n",
        "        # aggregate by redirected canonical URI\n",
        "        counts: Dict[str, int] = {}\n",
        "        for k, v in raw.items():\n",
        "            uri = self._redirect(k)\n",
        "            counts[uri] = counts.get(uri, 0) + int(v)\n",
        "        if not counts:\n",
        "            return []\n",
        "        max_support = max(counts.values()) or 1\n",
        "        items = [(uri, c / max_support) for uri, c in counts.items() if (c / max_support) >= thr]\n",
        "        items.sort(key=lambda x: x[1], reverse=True)\n",
        "        return items[:top_k]\n",
        "\n",
        "# =============== Predicate Retriever (precomputed) ===============\n",
        "\n",
        "class PredicateEmbeddingRetriever:\n",
        "    \"\"\"\n",
        "    Loads embeddings.npy (N, D) and predicates.csv (URIs line-by-line or CSV with 'predicate' column),\n",
        "    then retrieves top-K predicates for a relation text using cosine similarity.\n",
        "    Optional synonym expansion via Gemini.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_path: Optional[str] = None,\n",
        "        predicates_path: Optional[str] = None,\n",
        "        use_llm_synonyms: bool = True,\n",
        "        max_synonyms: int = 6,\n",
        "        llm_model_for_synonyms: str = \"gemini-2.5-flash\",\n",
        "        embed_model: str = \"embedding-001\",\n",
        "        verbose: bool = True,\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.embed_model = embed_model\n",
        "        self.use_llm_synonyms = use_llm_synonyms\n",
        "        self.max_synonyms = max_synonyms\n",
        "        self.llm_model_for_synonyms = llm_model_for_synonyms\n",
        "        self.verbose = verbose\n",
        "\n",
        "        emb_path, pred_path = self._find_files(embeddings_path, predicates_path)\n",
        "        self.E: np.ndarray = np.load(emb_path)  # (N, D)\n",
        "        self.predicates: List[str] = self._load_predicates(pred_path)\n",
        "        if self.E.shape[0] != len(self.predicates):\n",
        "            raise ValueError(f\"Row count mismatch: embeddings ({self.E.shape[0]}) vs predicates ({len(self.predicates)})\")\n",
        "\n",
        "        self.D = int(self.E.shape[1])\n",
        "        self.E_norm = self.E / (np.linalg.norm(self.E, axis=1, keepdims=True) + 1e-12)\n",
        "        self._syn_cache: Dict[Tuple[str, int], List[str]] = {}\n",
        "\n",
        "        if self.verbose:\n",
        "            _safe_print(f\"✓ Loaded embeddings: {emb_path} shape={self.E.shape}\")\n",
        "            _safe_print(f\"✓ Loaded predicates: {pred_path} count={len(self.predicates)}\")\n",
        "\n",
        "    def _find_files(self, emb_path: Optional[str], pred_path: Optional[str]) -> Tuple[str, str]:\n",
        "        if emb_path and pred_path and os.path.exists(emb_path) and os.path.exists(pred_path):\n",
        "            return emb_path, pred_path\n",
        "        cand_emb = [\"./embeddings.npy\", \"../embeddings.npy\", \"embeddings.npy\"]\n",
        "        cand_pred = [\"./predicates.csv\", \"../predicates.csv\", \"predicates.csv\"]\n",
        "        e = next((p for p in cand_emb if os.path.exists(p)), None)\n",
        "        p = next((q for q in cand_pred if os.path.exists(q)), None)\n",
        "        if not (e and p):\n",
        "            raise FileNotFoundError(f\"Could not find embeddings.npy and predicates.csv in {os.getcwd()} or ../\")\n",
        "        if self.verbose:\n",
        "            _safe_print(f\"✓ Found files: {e}, {p}\")\n",
        "        return e, p\n",
        "\n",
        "    def _load_predicates(self, path: str) -> List[str]:\n",
        "        \"\"\"Accepts either CSV with header including 'predicate' or raw newline list.\"\"\"\n",
        "        preds: List[str] = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            head = f.readline()\n",
        "            if \"predicate\" in head:\n",
        "                # CSV with header\n",
        "                for line in f:\n",
        "                    parts = line.rstrip(\"\\n\").split(\",\")\n",
        "                    if parts:\n",
        "                        preds.append(parts[0] if head.lower().startswith(\"predicate\") else parts[-1])\n",
        "            else:\n",
        "                # treat first line as data too\n",
        "                preds.append(head.strip())\n",
        "                for line in f:\n",
        "                    preds.append(line.strip())\n",
        "        # clean empties\n",
        "        preds = [p for p in preds if p]\n",
        "        return preds\n",
        "\n",
        "    def _synonyms_for(self, relation_text: str) -> List[str]:\n",
        "        if not self.use_llm_synonyms:\n",
        "            return [relation_text]\n",
        "        key = (relation_text.strip().lower(), self.max_synonyms)\n",
        "        if key in self._syn_cache:\n",
        "            return self._syn_cache[key]\n",
        "\n",
        "        prompt = (\n",
        "            f\"Generate {self.max_synonyms} short synonyms or verb-phrase paraphrases for a knowledge-graph relation.\\n\"\n",
        "            f'Return ONLY a JSON array of strings.\\n\\nRelation: \"{relation_text}\"'\n",
        "        )\n",
        "        out: List[str] = []\n",
        "        try:\n",
        "            resp = self.client.models.generate_content(\n",
        "                model=self.llm_model_for_synonyms,\n",
        "                contents=prompt,\n",
        "                config={\"response_mime_type\": \"application/json\"},\n",
        "            )\n",
        "            arr = _json_from_model(resp.text or \"[]\")\n",
        "            if isinstance(arr, list):\n",
        "                seen = set()\n",
        "                for s in arr:\n",
        "                    s = str(s).strip()\n",
        "                    if not s:\n",
        "                        continue\n",
        "                    k = s.lower()\n",
        "                    if k in seen:\n",
        "                        continue\n",
        "                    seen.add(k)\n",
        "                    out.append(s)\n",
        "                    if len(out) >= self.max_synonyms:\n",
        "                        break\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                _safe_print(f\"[synonyms] LLM failed for '{relation_text}': {e}\")\n",
        "\n",
        "        if not out:\n",
        "            base = relation_text.strip()\n",
        "            out = [base]\n",
        "        self._syn_cache[key] = out\n",
        "        if self.verbose:\n",
        "            _safe_print(f\"↪ Synonyms for '{relation_text}': {out}\")\n",
        "        return out\n",
        "\n",
        "    def _embed_texts(self, texts: Sequence[str]) -> np.ndarray:\n",
        "        cfg = types.EmbedContentConfig(output_dimensionality=int(self.D))\n",
        "        vecs: List[np.ndarray] = []\n",
        "        for t in texts:\n",
        "            resp = self.client.models.embed_content(model=self.embed_model, contents=t, config=cfg)\n",
        "            v = getattr(resp, \"embeddings\", None)\n",
        "            v = (v[0].values if v else resp.embedding.values)\n",
        "            vecs.append(_normalize(v))\n",
        "        return np.stack(vecs, axis=0)\n",
        "\n",
        "    def get_top_k_predicates(self, relation_text: str, top_k: int = 10) -> List[Tuple[str, float]]:\n",
        "        texts = self._synonyms_for(relation_text)\n",
        "        Q = self._embed_texts(texts)     # (S, D)\n",
        "        q = _normalize(Q.mean(axis=0))   # (D,)\n",
        "        sims = self.E_norm @ q           # (N,)\n",
        "        order = sims.argsort()[-top_k:][::-1]\n",
        "        return [(self.predicates[i], float(sims[i])) for i in order]\n",
        "\n",
        "# =============== LLM Disambiguator ===============\n",
        "\n",
        "class LLMDisambiguator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"gemini-2.5-flash\",\n",
        "        predicate_threshold: float = 0.5,\n",
        "        new_predicate_namespace: str = \"http://nef.local/rel/\",\n",
        "        verbose: bool = True,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.thr = float(predicate_threshold)\n",
        "        self.ns = new_predicate_namespace.rstrip(\"/\") + \"/\"\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _camelize(self, s: str) -> str:\n",
        "        import re\n",
        "        s = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", s).strip()\n",
        "        if not s: return \"relatedTo\"\n",
        "        parts = re.split(r\"\\s+\", s)\n",
        "        out = parts[0].lower() + \"\".join(p.capitalize() for p in parts[1:])\n",
        "        if out[0].isdigit(): out = \"rel\" + out\n",
        "        return out[:80]\n",
        "\n",
        "    def _mint_uri(self, local: str) -> str:\n",
        "        return self.ns + self._camelize(local)\n",
        "\n",
        "class LLMDisambiguator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"gemini-2.5-flash\",\n",
        "        predicate_threshold: float = 0.5,\n",
        "        new_predicate_namespace: str = \"http://nef.local/rel/\",\n",
        "        verbose: bool = True,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.thr = float(predicate_threshold)\n",
        "        self.ns = new_predicate_namespace.rstrip(\"/\") + \"/\"\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _camelize(self, s: str) -> str:\n",
        "        import re\n",
        "        s = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", s).strip()\n",
        "        if not s: return \"relatedTo\"\n",
        "        parts = re.split(r\"\\s+\", s)\n",
        "        out = parts[0].lower() + \"\".join(p.capitalize() for p in parts[1:])\n",
        "        if out[0].isdigit(): out = \"rel\" + out\n",
        "        return out[:80]\n",
        "\n",
        "    def _mint_uri(self, local: str) -> str:\n",
        "        return self.ns + self._camelize(local)\n",
        "\n",
        "    def disambiguate_triple(\n",
        "        self,\n",
        "        context: str,\n",
        "        subject_candidates,      # List[(uri, score)]\n",
        "        predicate_candidates,    # List[(uri, sim)] sorted desc\n",
        "        object_candidates,       # List[(uri, score)]\n",
        "    ):\n",
        "        total_k = len(predicate_candidates)\n",
        "        sim_map = {u: (s, i) for i, (u, s) in enumerate(predicate_candidates)}\n",
        "\n",
        "        # Split by threshold\n",
        "        above = [(u, s) for (u, s) in predicate_candidates if s >= self.thr]\n",
        "\n",
        "        if above:\n",
        "            allowed = [u for u, _ in above]\n",
        "            pred_list_text = \"\\n\".join([f\"- {u}\" for u in allowed])\n",
        "            prompt = f\"\"\"Pick the best RDF triple using ONLY these predicate URIs.\n",
        "\n",
        "Allowed predicates:\n",
        "{pred_list_text}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Return ONLY JSON: {{\"subject\":\"URI\",\"predicate\":\"URI\",\"object\":\"URI\"}}\n",
        "\"\"\"\n",
        "            try:\n",
        "                resp = client.models.generate_content(\n",
        "                    model=self.model_name,\n",
        "                    contents=prompt,\n",
        "                    config={\"response_mime_type\": \"application/json\"},\n",
        "                )\n",
        "                data = _json_from_model(resp.text or \"{}\")\n",
        "                pred_uri = data.get(\"predicate\", \"\")\n",
        "                if pred_uri not in allowed:\n",
        "                    # Force top allowed if the model drifts\n",
        "                    pred_uri = allowed[0]\n",
        "\n",
        "                chosen_sim, rank0 = sim_map.get(pred_uri, (None, None))\n",
        "                meta = {\n",
        "                    \"label\": \"candidate\",\n",
        "                    \"chosen_similarity\": float(chosen_sim) if chosen_sim is not None else None,\n",
        "                    \"rank_in_topk\": (rank0 + 1) if rank0 is not None else None,\n",
        "                    \"topk\": total_k,\n",
        "                    \"threshold\": self.thr,\n",
        "                }\n",
        "                return (\n",
        "                    data.get(\"subject\", subject_candidates[0][0] if subject_candidates else \"\"),\n",
        "                    pred_uri,\n",
        "                    data.get(\"object\", object_candidates[0][0] if object_candidates else \"\"),\n",
        "                    meta,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"✗ disambiguation error; using top allowed: {e}\")\n",
        "                best_uri, best_sim = max(above, key=lambda x: x[1])\n",
        "                _, rank0 = sim_map.get(best_uri, (best_sim, 0))\n",
        "                meta = {\n",
        "                    \"label\": \"candidate\",\n",
        "                    \"chosen_similarity\": float(best_sim),\n",
        "                    \"rank_in_topk\": (rank0 + 1),\n",
        "                    \"topk\": total_k,\n",
        "                    \"threshold\": self.thr,\n",
        "                }\n",
        "                return (\n",
        "                    subject_candidates[0][0] if subject_candidates else \"\",\n",
        "                    best_uri,\n",
        "                    object_candidates[0][0] if object_candidates else \"\",\n",
        "                    meta,\n",
        "                )\n",
        "\n",
        "        # None ≥ threshold → generate\n",
        "        prompt = f\"\"\"No predicate meets the threshold ({self.thr:.2f}).\n",
        "Propose a NEW concise camelCase predicate name for this relation in context.\n",
        "Return ONLY JSON: {{\"predicateLocalName\":\"camelCase\"}}.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "        try:\n",
        "            resp = client.models.generate_content(\n",
        "                model=self.model_name,\n",
        "                contents=prompt,\n",
        "                config={\"response_mime_type\": \"application/json\"},\n",
        "            )\n",
        "            data = _json_from_model(resp.text or \"{}\")\n",
        "            local = (data.get(\"predicateLocalName\") or \"relatedTo\").strip()\n",
        "            pred_uri = self._mint_uri(local)\n",
        "            meta = {\n",
        "                \"label\": \"generated\",\n",
        "                \"chosen_similarity\": None,\n",
        "                \"rank_in_topk\": None,\n",
        "                \"topk\": total_k,\n",
        "                \"threshold\": self.thr,\n",
        "            }\n",
        "            return (\n",
        "                subject_candidates[0][0] if subject_candidates else \"\",\n",
        "                pred_uri,\n",
        "                object_candidates[0][0] if object_candidates else \"\",\n",
        "                meta,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"✗ generation error; minting default: {e}\")\n",
        "            pred_uri = self._mint_uri(\"relatedTo\")\n",
        "            meta = {\n",
        "                \"label\": \"generated\",\n",
        "                \"chosen_similarity\": None,\n",
        "                \"rank_in_topk\": None,\n",
        "                \"topk\": total_k,\n",
        "                \"threshold\": self.thr,\n",
        "            }\n",
        "            return (\n",
        "                subject_candidates[0][0] if subject_candidates else \"\",\n",
        "                pred_uri,\n",
        "                object_candidates[0][0] if object_candidates else \"\",\n",
        "                meta,\n",
        "            )\n",
        "\n",
        "\n",
        "# =============== Orchestrator ===============\n",
        "\n",
        "class EnhancedNEFPipeline:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline:\n",
        "      1) Extract triples with Gemini\n",
        "      2) Redis entity linking (subject/object)\n",
        "      3) Predicate retrieval via precomputed embeddings\n",
        "      4) LLM disambiguation\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_path: Optional[str] = None,\n",
        "        predicates_path: Optional[str] = None,\n",
        "        llm_model: str = \"gemini-2.5-flash\",\n",
        "        use_llm_synonyms: bool = True,\n",
        "        verbose: bool = True,\n",
        "    ):\n",
        "        self.verbose = verbose\n",
        "        self.redis_el = RedisEntityLinking()\n",
        "        self.pred = PredicateEmbeddingRetriever(\n",
        "            embeddings_path=embeddings_path,\n",
        "            predicates_path=predicates_path,\n",
        "            use_llm_synonyms=use_llm_synonyms,\n",
        "            max_synonyms=6,\n",
        "            llm_model_for_synonyms=llm_model,\n",
        "            embed_model=\"embedding-001\",\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        self.llm = LLMDisambiguator(llm_model)\n",
        "        if self.verbose:\n",
        "            _safe_print(\"✓ Enhanced NEF Pipeline initialized\")\n",
        "\n",
        "    def _extract_triples(self, text: str) -> list[dict]:\n",
        "        \"\"\"\n",
        "        Simple extractor: asks for up to 5 triples with confidence,\n",
        "        then filters: confidence≥0.5, looks-like-named, Redis-groundable.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "    Return up to 5 RDF triples from the text as ONLY JSON:\n",
        "    [{{\"subject\":\"...\", \"predicate\":\"...\", \"object\":\"...\", \"confidence\":0.0}}, ...]\n",
        "\n",
        "    Rules:\n",
        "    - subject & object: exact surface forms from the text, concrete named entities (no pronouns/generic phrases).\n",
        "    - predicate: short verb/verb-phrase from the text.\n",
        "    - confidence in [0,1]. Only include triples you are ≥0.5 confident in.\n",
        "\n",
        "    Text: {text}\n",
        "    \"\"\".strip()\n",
        "\n",
        "        try:\n",
        "            resp = client.models.generate_content(\n",
        "                model=\"gemini-2.5-flash\",\n",
        "                contents=prompt,\n",
        "                config={\"response_mime_type\": \"application/json\"},\n",
        "            )\n",
        "            items = _json_from_model(resp.text or \"[]\")\n",
        "            if not isinstance(items, list):\n",
        "                return []\n",
        "\n",
        "            def _looks_named(x: str) -> bool:\n",
        "                toks = x.split()\n",
        "                return any(t and (t[0].isupper() or t.isupper()) for t in toks)\n",
        "\n",
        "            out, seen = [], set()\n",
        "            for it in items:\n",
        "                s = (it.get(\"subject\") or \"\").strip()\n",
        "                p = (it.get(\"predicate\") or \"\").strip()\n",
        "                o = (it.get(\"object\") or \"\").strip()\n",
        "                conf = float(it.get(\"confidence\") or 0.0)\n",
        "\n",
        "                if not (s and p and o):                   # all fields present\n",
        "                    continue\n",
        "                if conf < 0.5:                            # confidence gate\n",
        "                    continue\n",
        "                if not (_looks_named(s) and _looks_named(o)):  # avoid generic endpoints\n",
        "                    continue\n",
        "\n",
        "                # Redis grounding (no generation)\n",
        "                if not self._resolve_entities(s, k=1) or not self._resolve_entities(o, k=1):\n",
        "                    continue\n",
        "\n",
        "                key = (s, p, o)\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                out.append({\"subject\": s, \"predicate\": p, \"object\": o})\n",
        "                if len(out) >= 5:\n",
        "                    break\n",
        "\n",
        "            return out\n",
        "\n",
        "        except Exception as e:\n",
        "            if getattr(self, \"verbose\", True):\n",
        "                _safe_print(f\"✗ Triple extraction error: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "\n",
        "    def _resolve_entities(self, mention: str, k: int = 5) -> List[Tuple[str, float]]:\n",
        "        cands = self.redis_el.lookup(mention, top_k=k)\n",
        "        # ensure URIs\n",
        "        fixed: List[Tuple[str, float]] = []\n",
        "        for uri, score in cands:\n",
        "            if not (uri.startswith(\"http://\") or uri.startswith(\"https://\")):\n",
        "                uri = f\"http://dbpedia.org/resource/{quote(uri)}\"\n",
        "            fixed.append((uri, score))\n",
        "        return fixed\n",
        "\n",
        "    def run_pipeline(self, sentence: str) -> list[tuple[str, str, str]]:\n",
        "        \"\"\"\n",
        "        End-to-end for one sentence.\n",
        "        Prints minimal tag [CANDIDATE] or [GENERATED] plus sim, rank/topk, thr.\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            _safe_print(f\"\\n📝 {sentence!r}\")\n",
        "\n",
        "        raw_triples = self._extract_triples(sentence)\n",
        "        if not raw_triples:\n",
        "            if self.verbose:\n",
        "                _safe_print(\"   ⚠ No triples extracted.\")\n",
        "            return []\n",
        "\n",
        "        results: list[tuple[str, str, str]] = []\n",
        "\n",
        "        for t in raw_triples:\n",
        "            s = (t.get(\"subject\") or \"\").strip()\n",
        "            p = (t.get(\"predicate\") or \"\").strip()\n",
        "            o = (t.get(\"object\") or \"\").strip()\n",
        "            if not (s and p and o):\n",
        "                continue\n",
        "\n",
        "            if self.verbose:\n",
        "                _safe_print(f\"\\n🔍 Triple: {s} — {p} — {o}\")\n",
        "                _safe_print(\"   📍 Getting entity candidates from Redis...\")\n",
        "\n",
        "            subject_candidates = self._resolve_entities(s, k=5)\n",
        "            object_candidates  = self._resolve_entities(o, k=5)\n",
        "\n",
        "            if self.verbose:\n",
        "                _safe_print(\"   [Redis:subject]\", subject_candidates[:5] if subject_candidates else \"NO CANDIDATES\")\n",
        "                _safe_print(\"   [Redis:object]\",  object_candidates[:5]  if object_candidates  else \"NO CANDIDATES\")\n",
        "\n",
        "            if not subject_candidates or not object_candidates:\n",
        "                if self.verbose:\n",
        "                    _safe_print(\"   ⚠ Abandoning triple (no Redis candidates for subject/object).\")\n",
        "                continue\n",
        "\n",
        "            predicate_candidates = self.pred.get_top_k_predicates(p, top_k=10)\n",
        "            if self.verbose:\n",
        "                _safe_print(\"   [Predicates:top5]\", predicate_candidates[:5])\n",
        "\n",
        "            s_final, p_final, o_final, meta = self.llm.disambiguate_triple(\n",
        "                sentence, subject_candidates, predicate_candidates, object_candidates\n",
        "            )\n",
        "            results.append((s_final, p_final, o_final))\n",
        "\n",
        "            # Minimal tag but rich metrics\n",
        "            label = (meta or {}).get(\"label\", \"candidate\")\n",
        "            tag_str = \"[GENERATED]\" if label == \"generated\" else \"[CANDIDATE]\"\n",
        "\n",
        "            sim = (meta or {}).get(\"chosen_similarity\")\n",
        "            rank = (meta or {}).get(\"rank_in_topk\")\n",
        "            topk = (meta or {}).get(\"topk\")\n",
        "            thr = (meta or {}).get(\"threshold\")\n",
        "\n",
        "            sim_str = f\" (sim={sim:.3f})\" if isinstance(sim, (int, float)) else \"\"\n",
        "            rank_str = f\" rank={rank}/{topk}\" if (isinstance(rank, int) and isinstance(topk, int)) else \"\"\n",
        "            thr_str = f\" | thr={thr:.2f}\" if isinstance(thr, (int, float)) else \"\"\n",
        "\n",
        "            if self.verbose:\n",
        "                _safe_print(\"   ✅ Final\", f\"{tag_str}{sim_str}{rank_str}{thr_str}:\", s_final, p_final, o_final, sep=\"\\n            \")\n",
        "\n",
        "        return results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Om39VT1BSbVl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🚀 TESTING ENHANCED NEF PIPELINE (precomputed embeddings)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "pipeline = EnhancedNEFPipeline(\n",
        "    embeddings_path=None,\n",
        "    predicates_path=None,\n",
        "    llm_model=\"gemini-2.5-flash\",\n",
        "    use_llm_synonyms=False\n",
        ")\n",
        "\n",
        "test_sentences = [\n",
        "    \"Einstein was born in Ulm\",\n",
        "    \"Steve Jobs founded Apple\",\n",
        "    \"Marie Curie discovered radium\",\n",
        "    \"Shakespeare wrote Hamlet\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for s in test_sentences:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    triples = pipeline.run_pipeline(s)\n",
        "    results.append({\"sentence\": s, \"triples\": triples})\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lscPxJ0U5dS2",
        "outputId": "92b1c6da-1285-4197-9553-3f81f0572a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "🚀 TESTING ENHANCED NEF PIPELINE (precomputed embeddings)\n",
            "================================================================================\n",
            "✓ Connected to Redis\n",
            "✓ Found files: ./embeddings.npy, ./predicates.csv\n",
            "✓ Loaded embeddings: ./embeddings.npy shape=(2884, 768)\n",
            "✓ Loaded predicates: ./predicates.csv count=2884\n",
            "✓ Enhanced NEF Pipeline initialized\n",
            "\n",
            "================================================================================\n",
            "\n",
            "📝 'Einstein was born in Ulm'\n",
            "\n",
            "🔍 Triple: Einstein — was born in — Ulm\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Albert_Einstein', 1.0), ('http://dbpedia.org/resource/Einstein_%28horse%29', 0.05152224824355972), ('http://dbpedia.org/resource/Einstein_%28film%29', 0.0234192037470726), ('http://dbpedia.org/resource/Einstein_%28German_TV_series%29', 0.0234192037470726), ('http://dbpedia.org/resource/Einstein_%28crater%29', 0.02107728337236534)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Ulm', 1.0), ('http://dbpedia.org/resource/Ulm_Hauptbahnhof', 0.07557732680195942), ('http://dbpedia.org/resource/Ratiopharm_Ulm', 0.05808257522743177), ('http://dbpedia.org/resource/Battle_of_Ulm', 0.026592022393282014), ('http://dbpedia.org/resource/Neu-Ulm_Challenger', 0.013995801259622114)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/birthPlace', 0.7748733162879944), ('http://dbpedia.org/ontology/birthYear', 0.7524516582489014), ('http://dbpedia.org/ontology/birthDate', 0.7498449087142944), ('http://dbpedia.org/ontology/birthName', 0.735774040222168), ('http://dbpedia.org/ontology/birthSign', 0.711251974105835)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.775) rank=1/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Albert_Einstein\n",
            "            http://dbpedia.org/ontology/birthPlace\n",
            "            http://dbpedia.org/resource/Ulm\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "📝 'Steve Jobs founded Apple'\n",
            "\n",
            "🔍 Triple: Steve Jobs — founded — Apple\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Steve_Jobs', 1.0), ('http://dbpedia.org/resource/Steve_Jobs_%28film%29', 0.15726495726495726), ('http://dbpedia.org/resource/Steve_Jobs_%28book%29', 0.023076923076923078)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Apple_Inc.', 1.0), ('http://dbpedia.org/resource/Apple_Records', 0.11535756154747949), ('http://dbpedia.org/resource/Apple', 0.05955451348182884), ('http://dbpedia.org/resource/Apple_Corps', 0.01875732708089097)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/foundingYear', 0.7635200023651123), ('http://dbpedia.org/ontology/foundingDate', 0.7563939094543457), ('http://dbpedia.org/ontology/foundedBy', 0.7447636127471924), ('http://dbpedia.org/ontology/foundation', 0.7380497455596924), ('http://dbpedia.org/ontology/foundationPlace', 0.711121141910553)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.711) rank=6/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Steve_Jobs\n",
            "            http://dbpedia.org/ontology/founder\n",
            "            http://dbpedia.org/resource/Apple_Inc.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "📝 'Marie Curie discovered radium'\n",
            "   ⚠ No triples extracted.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "📝 'Shakespeare wrote Hamlet'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# FAST \"NEW KNOWLEDGE\" SCRAPER (recency + provenance)\n",
        "# - Pulls recent items (default: last 48h) across topics\n",
        "# - Keeps: category, feed, title, url, published_iso, snippet (cleaned)\n",
        "# - Dedupe by URL; respects robots.txt; uses readability for main text\n",
        "# - Target: ~10 items/topic from up to 2 feeds/topic (tunable)\n",
        "# ================================================\n",
        "\n",
        "# Install deps (quiet)\n",
        "!pip -q install feedparser readability-lxml beautifulsoup4 lxml tldextract\n",
        "\n",
        "import time, csv, re, sys, email.utils as eut\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import feedparser, requests\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document\n",
        "import urllib.robotparser as urobot\n",
        "\n",
        "# ----------------- SETTINGS -----------------\n",
        "HEADERS = {\"User-Agent\": \"NEF-RecentScraper/1.0 (+contact@example.com)\"}\n",
        "TIMEOUT = 15\n",
        "MAX_RETRIES = 1\n",
        "SLEEP_BETWEEN_REQUESTS = 0.25  # seconds between article fetches\n",
        "\n",
        "# Content thresholds\n",
        "MIN_SNIPPET_CHARS = 140         # after cleaning\n",
        "MAX_SNIPPET_CHARS = 900         # clamp long bodies\n",
        "\n",
        "# Collection targets\n",
        "FEEDS_PER_TOPIC   = 2\n",
        "MAX_PER_FEED      = 6\n",
        "TARGET_PER_TOPIC  = 10\n",
        "RECENCY_HOURS     = 48          # only keep items published/updated in this window\n",
        "\n",
        "# ----------------- TOPIC FEEDS -----------------\n",
        "FEEDS = {\n",
        "    \"TV shows & movies\": [\n",
        "        \"https://www.theguardian.com/film/rss\",\n",
        "        \"https://variety.com/v/tv/feed/\",\n",
        "        \"https://www.hollywoodreporter.com/c/tv/tv-news/feed/\",\n",
        "    ],\n",
        "    \"Science & technology\": [\n",
        "        \"https://arstechnica.com/feed/\",\n",
        "        \"https://www.theverge.com/rss/index.xml\",\n",
        "        \"https://www.npr.org/rss/rss.php?id=1019\",\n",
        "    ],\n",
        "    \"Art\": [\n",
        "        \"https://hyperallergic.com/feed/\",\n",
        "        \"https://news.artnet.com/feed\",\n",
        "        \"https://www.theguardian.com/artanddesign/rss\",\n",
        "    ],\n",
        "    \"History\": [\n",
        "        \"https://www.historyextra.com/feed/\",\n",
        "        \"https://www.theguardian.com/culture/series/thelongread/rss\",\n",
        "    ],\n",
        "    \"Sports\": [\n",
        "        \"https://www.espn.com/espn/rss/news\",\n",
        "        \"https://feeds.bbci.co.uk/sport/rss.xml\",\n",
        "        \"https://www.npr.org/rss/rss.php?id=1055\",\n",
        "    ],\n",
        "    \"Music\": [\n",
        "        \"https://www.theguardian.com/music/rss\",\n",
        "        \"https://pitchfork.com/feed/feed-news/rss\",\n",
        "        \"https://www.npr.org/rss/rss.php?id=1039\",\n",
        "    ],\n",
        "    \"Video games\": [\n",
        "        \"https://feeds.ign.com/ign/games-all\",\n",
        "        \"https://www.polygon.com/rss/index.xml\",\n",
        "        \"https://www.pcgamer.com/rss/\",\n",
        "    ],\n",
        "    \"Geography\": [\n",
        "        \"https://www.atlasobscura.com/feeds/latest\",\n",
        "        \"https://www.bbc.com/travel/rss\",\n",
        "        \"https://earthobservatory.nasa.gov/feeds/rss/eo.rss\",\n",
        "    ],\n",
        "    \"Politics\": [\n",
        "        \"https://feeds.bbci.co.uk/news/politics/rss.xml\",\n",
        "        \"https://www.theguardian.com/politics/rss\",\n",
        "        \"https://www.politico.com/rss/politics-news.xml\",\n",
        "    ],\n",
        "    \"Other\": [\n",
        "        \"https://www.theguardian.com/world/rss\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# ----------------- HELPERS -----------------\n",
        "def allowed_by_robots(url: str) -> bool:\n",
        "    \"\"\"Check robots.txt for article URLs; fall back to allow on errors.\"\"\"\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
        "        rp = urobot.RobotFileParser()\n",
        "        rp.set_url(robots_url)\n",
        "        rp.read()\n",
        "        return rp.can_fetch(HEADERS[\"User-Agent\"], url)\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "SESSION = requests.Session()\n",
        "SESSION.headers.update(HEADERS)\n",
        "\n",
        "def fetch_html(url: str) -> str | None:\n",
        "    \"\"\"Fetch HTML (skip XML feeds/pages); retry lightly; honor robots.\"\"\"\n",
        "    if not allowed_by_robots(url):\n",
        "        return None\n",
        "    for _ in range(MAX_RETRIES + 1):\n",
        "        try:\n",
        "            r = SESSION.get(url, timeout=TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "            ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "            if \"xml\" in ctype:\n",
        "                return None\n",
        "            return r.text\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def extract_main_text(html: str) -> tuple[str, str]:\n",
        "    \"\"\"Prefer Readability; fallback to basic soup stripping.\"\"\"\n",
        "    if not html:\n",
        "        return \"\", \"\"\n",
        "    try:\n",
        "        doc = Document(html)\n",
        "        title = (doc.short_title() or \"\").strip()\n",
        "        soup = BeautifulSoup(doc.summary(html_partial=True), \"lxml\")\n",
        "        text = \" \".join(soup.stripped_strings)\n",
        "        return title, text\n",
        "    except Exception:\n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "        title = (soup.title.string if soup.title else \"\").strip()\n",
        "        for tag in soup([\"script\",\"style\",\"header\",\"footer\",\"nav\",\"aside\"]):\n",
        "            tag.decompose()\n",
        "        text = \" \".join(soup.stripped_strings)\n",
        "        return title, text\n",
        "\n",
        "def clean_snippet(txt: str, limit=MAX_SNIPPET_CHARS) -> str:\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt or \"\").strip()\n",
        "    return txt[:limit]\n",
        "\n",
        "def entry_summary_fallback(entry) -> str:\n",
        "    summary = entry.get(\"summary\", \"\") or entry.get(\"description\", \"\")\n",
        "    soup = BeautifulSoup(summary, \"lxml\")\n",
        "    return clean_snippet(\" \".join(soup.stripped_strings))\n",
        "\n",
        "def parse_entry_dt(entry) -> datetime | None:\n",
        "    \"\"\"Parse RFC822-like dates if available; return aware UTC datetime.\"\"\"\n",
        "    ts = entry.get(\"published\") or entry.get(\"updated\") or entry.get(\"dc:date\")\n",
        "    if not ts:\n",
        "        return None\n",
        "    try:\n",
        "        dt = eut.parsedate_to_datetime(ts)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def iso_or_empty(dt: datetime | None) -> str:\n",
        "    return dt.astimezone(timezone.utc).isoformat() if dt else \"\"\n",
        "\n",
        "# ----------------- MAIN SCRAPER -----------------\n",
        "def scrape_feeds_recent(feeds_by_topic: dict) -> list[dict]:\n",
        "    rows, seen_urls = [], set()\n",
        "    now_utc = datetime.now(timezone.utc)\n",
        "    recency_delta = timedelta(hours=RECENCY_HOURS)\n",
        "\n",
        "    for topic, feeds in feeds_by_topic.items():\n",
        "        topic_rows = 0\n",
        "        print(f\"\\n[topic] {topic} → target {TARGET_PER_TOPIC} items from up to {min(FEEDS_PER_TOPIC, len(feeds))} feeds\")\n",
        "\n",
        "        for i, feed_url in enumerate(feeds[:FEEDS_PER_TOPIC], start=1):\n",
        "            if topic_rows >= TARGET_PER_TOPIC:\n",
        "                break\n",
        "            print(f\"  [feed {i}] {feed_url}\", flush=True)\n",
        "\n",
        "            feed = feedparser.parse(feed_url, request_headers=HEADERS)\n",
        "            if feed.bozo and not feed.entries:\n",
        "                print(f\"    [warn] broken feed\")\n",
        "                continue\n",
        "\n",
        "            kept_from_feed = 0\n",
        "            for e in feed.entries:\n",
        "                if kept_from_feed >= MAX_PER_FEED or topic_rows >= TARGET_PER_TOPIC:\n",
        "                    break\n",
        "\n",
        "                url = e.get(\"link\")\n",
        "                if not url or url in seen_urls:\n",
        "                    continue\n",
        "\n",
        "                # Recency filter (when date available)\n",
        "                dt = parse_entry_dt(e)\n",
        "                if dt is not None and (now_utc - dt) > recency_delta:\n",
        "                    continue  # too old for \"new knowledge\" testing\n",
        "\n",
        "                html = fetch_html(url)\n",
        "                title, text = extract_main_text(html) if html else (\"\", \"\")\n",
        "                if not title:\n",
        "                    title = (e.get(\"title\") or \"\").strip()\n",
        "\n",
        "                snippet = clean_snippet(text)\n",
        "                if len(snippet) < MIN_SNIPPET_CHARS:\n",
        "                    snippet = entry_summary_fallback(e)\n",
        "\n",
        "                if not title or len(snippet) < MIN_SNIPPET_CHARS:\n",
        "                    continue\n",
        "\n",
        "                rows.append({\n",
        "                    \"category\": topic,\n",
        "                    \"feed\": feed_url,\n",
        "                    \"title\": title,\n",
        "                    \"url\": url,\n",
        "                    \"published_iso\": iso_or_empty(dt),\n",
        "                    \"snippet\": snippet,\n",
        "                })\n",
        "                seen_urls.add(url)\n",
        "                kept_from_feed += 1\n",
        "                topic_rows += 1\n",
        "                print(f\"    [+] {kept_from_feed}/{MAX_PER_FEED} from feed | {topic_rows}/{TARGET_PER_TOPIC} in topic\", flush=True)\n",
        "                time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "\n",
        "            print(f\"  [done] kept {kept_from_feed} from this feed\")\n",
        "\n",
        "        print(f\"[topic done] {topic}: kept {topic_rows}/{TARGET_PER_TOPIC}\")\n",
        "\n",
        "    return rows\n",
        "\n",
        "# ----------------- RUN -----------------\n",
        "rows = scrape_feeds_recent(FEEDS)\n",
        "\n",
        "out_path = \"recent_seeds_categorized.csv\"\n",
        "with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"category\",\"feed\",\"title\",\"url\",\"published_iso\",\"snippet\"])\n",
        "    w.writeheader()\n",
        "    w.writerows(rows)\n",
        "\n",
        "print(f\"\\n✅ Saved {len(rows)} items → {out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rur6cAQLELmt",
        "outputId": "75f137f5-621c-4885-89aa-f96f3c534f39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "[topic] TV shows & movies → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://www.theguardian.com/film/rss\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://variety.com/v/tv/feed/\n",
            "    [+] 1/6 from feed | 7/10 in topic\n",
            "    [+] 2/6 from feed | 8/10 in topic\n",
            "    [+] 3/6 from feed | 9/10 in topic\n",
            "    [+] 4/6 from feed | 10/10 in topic\n",
            "  [done] kept 4 from this feed\n",
            "[topic done] TV shows & movies: kept 10/10\n",
            "\n",
            "[topic] Science & technology → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://arstechnica.com/feed/\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://www.theverge.com/rss/index.xml\n",
            "    [+] 1/6 from feed | 7/10 in topic\n",
            "    [+] 2/6 from feed | 8/10 in topic\n",
            "    [+] 3/6 from feed | 9/10 in topic\n",
            "    [+] 4/6 from feed | 10/10 in topic\n",
            "  [done] kept 4 from this feed\n",
            "[topic done] Science & technology: kept 10/10\n",
            "\n",
            "[topic] Art → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://hyperallergic.com/feed/\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://news.artnet.com/feed\n",
            "    [+] 1/6 from feed | 7/10 in topic\n",
            "    [+] 2/6 from feed | 8/10 in topic\n",
            "    [+] 3/6 from feed | 9/10 in topic\n",
            "    [+] 4/6 from feed | 10/10 in topic\n",
            "  [done] kept 4 from this feed\n",
            "[topic done] Art: kept 10/10\n",
            "\n",
            "[topic] History → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://www.historyextra.com/feed/\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://www.theguardian.com/culture/series/thelongread/rss\n",
            "    [warn] broken feed\n",
            "[topic done] History: kept 6/10\n",
            "\n",
            "[topic] Sports → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://www.espn.com/espn/rss/news\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://feeds.bbci.co.uk/sport/rss.xml\n",
            "    [+] 1/6 from feed | 7/10 in topic\n",
            "    [+] 2/6 from feed | 8/10 in topic\n",
            "    [+] 3/6 from feed | 9/10 in topic\n",
            "    [+] 4/6 from feed | 10/10 in topic\n",
            "  [done] kept 4 from this feed\n",
            "[topic done] Sports: kept 10/10\n",
            "\n",
            "[topic] Music → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://www.theguardian.com/music/rss\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://pitchfork.com/feed/feed-news/rss\n",
            "    [+] 1/6 from feed | 7/10 in topic\n",
            "    [+] 2/6 from feed | 8/10 in topic\n",
            "    [+] 3/6 from feed | 9/10 in topic\n",
            "    [+] 4/6 from feed | 10/10 in topic\n",
            "  [done] kept 4 from this feed\n",
            "[topic done] Music: kept 10/10\n",
            "\n",
            "[topic] Video games → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://feeds.ign.com/ign/games-all\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://www.polygon.com/rss/index.xml\n",
            "    [+] 1/6 from feed | 7/10 in topic\n",
            "    [+] 2/6 from feed | 8/10 in topic\n",
            "    [+] 3/6 from feed | 9/10 in topic\n",
            "    [+] 4/6 from feed | 10/10 in topic\n",
            "  [done] kept 4 from this feed\n",
            "[topic done] Video games: kept 10/10\n",
            "\n",
            "[topic] Geography → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://www.atlasobscura.com/feeds/latest\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://www.bbc.com/travel/rss\n",
            "    [warn] broken feed\n",
            "[topic done] Geography: kept 6/10\n",
            "\n",
            "[topic] Politics → target 10 items from up to 2 feeds\n",
            "  [feed 1] https://feeds.bbci.co.uk/news/politics/rss.xml\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "  [feed 2] https://www.theguardian.com/politics/rss\n",
            "    [+] 1/6 from feed | 7/10 in topic\n",
            "    [+] 2/6 from feed | 8/10 in topic\n",
            "    [+] 3/6 from feed | 9/10 in topic\n",
            "    [+] 4/6 from feed | 10/10 in topic\n",
            "  [done] kept 4 from this feed\n",
            "[topic done] Politics: kept 10/10\n",
            "\n",
            "[topic] Other → target 10 items from up to 1 feeds\n",
            "  [feed 1] https://www.theguardian.com/world/rss\n",
            "    [+] 1/6 from feed | 1/10 in topic\n",
            "    [+] 2/6 from feed | 2/10 in topic\n",
            "    [+] 3/6 from feed | 3/10 in topic\n",
            "    [+] 4/6 from feed | 4/10 in topic\n",
            "    [+] 5/6 from feed | 5/10 in topic\n",
            "    [+] 6/6 from feed | 6/10 in topic\n",
            "  [done] kept 6 from this feed\n",
            "[topic done] Other: kept 6/10\n",
            "\n",
            "✅ Saved 88 items → recent_seeds_categorized.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TEST: Run Enhanced NEF Pipeline on new-knowledge CSV\n",
        "# - Reads recent_seeds_categorized.csv (output from scraper)\n",
        "# - For each row, runs triple extraction + resolution\n",
        "# - Logs provenance (category, title, URL, published_iso)\n",
        "# - Records support sentence and final resolved triple\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd, json, time\n",
        "\n",
        "csv_path = \"recent_seeds_categorized.csv\"\n",
        "df = pd.read_csv(csv_path).dropna(subset=[\"snippet\"])\n",
        "print(f\"Loaded {len(df)} snippets from {csv_path}\")\n",
        "\n",
        "# limit to e.g. 10 recent items for quick test\n",
        "sample = df.sample(n=min(10, len(df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, row in sample.iterrows():\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"[{i+1}/{len(sample)}] {row['category']} | {row['title']}\")\n",
        "    print(f\"URL: {row['url']}\")\n",
        "    print(f\"Published: {row.get('published_iso','')}\")\n",
        "    print(\"-\"*100)\n",
        "\n",
        "    snippet = row[\"snippet\"]\n",
        "    start = time.time()\n",
        "    triples = pipeline.run_pipeline(snippet)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    for t in triples:\n",
        "        if len(t) == 4:\n",
        "            subj, pred, obj, reason = t\n",
        "        elif len(t) == 3:\n",
        "            subj, pred, obj = t\n",
        "            reason = \"\"  # or \"N/A\"\n",
        "        else:\n",
        "            # Unexpected shape; skip\n",
        "            continue\n",
        "\n",
        "        results.append({\n",
        "            \"category\": row[\"category\"],\n",
        "            \"title\": row[\"title\"],\n",
        "            \"url\": row[\"url\"],\n",
        "            \"published_iso\": row.get(\"published_iso\",\"\"),\n",
        "            \"subject_uri\": subj,\n",
        "            \"predicate_uri\": pred,\n",
        "            \"object_uri\": obj,\n",
        "            \"reasoning\": reason,\n",
        "            \"snippet\": snippet[:300] + (\"...\" if len(snippet) > 300 else \"\"),\n",
        "            \"runtime_sec\": round(elapsed, 2)\n",
        "        })\n",
        "\n",
        "# save detailed output\n",
        "out_path = \"triple_results_recent.csv\"\n",
        "pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "print(f\"\\n✅ Saved {len(results)} resolved triples → {out_path}\")\n",
        "\n",
        "# quick glance\n",
        "pd.DataFrame(results)[[\"title\",\"predicate_uri\",\"subject_uri\",\"object_uri\"]].head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H7G0FZ8CFaHd",
        "outputId": "c4b5d289-73ea-48eb-8fd0-13708ad6a073"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 88 snippets from recent_seeds_categorized.csv\n",
            "\n",
            "====================================================================================================\n",
            "[1/10] Politics | Cabinet Office rejects Cummings' claim that China breached high-level systems\n",
            "URL: https://www.bbc.com/news/articles/ce3xz607dpro?at_medium=RSS&at_campaign=rss\n",
            "Published: 2025-10-16T04:19:49+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'The Cabinet Office has rejected Dominic Cummings\\' claim that China breached high-level systems used to transfer sensitive government information. In an interview with the Times , Cummings said China obtained \"vast amounts\" of \"extremely secret\" information from the UK intelligence services and parts of Whitehall. He told the paper the breach was covered up after he was briefed on the compromised data in 2020 while a senior aide to then-Prime Minister Boris Johnson. In response, a Cabinet Office spokesperson said: \"It is untrue to claim that the systems we use to transfer the most sensitive government information have been compromised.\" Cummings said China breached high-level systems used to transfer so-called Strap material, a government classification for highly sensitive intelligence data. In the interview, he said the compromised information included: \"Material from intelligence servi'\n",
            "   ⚠ No triples extracted.\n",
            "\n",
            "====================================================================================================\n",
            "[2/10] TV shows & movies | Back of the net! Steve Coogan film roles - ranked\n",
            "URL: https://www.theguardian.com/film/2025/oct/16/back-of-the-net-steve-coogan-film-performances-ranked\n",
            "Published: 2025-10-16T12:46:47+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'Steve Coogan entered the unofficial British comedy hall of fame at the tender age of 30, when he joined four of the Pythons and the likes of Stephen Fry and Victoria Wood in this largely forgotten version of the classic children’s tale. Bedecked in a long scarf and wearing rimless specs, he puts in a respectably twitchy turn as Mole alongside Eric Idle’s Rat and Terry Jones’s Toad. In the comedy franchise about museum exhibits coming to life at night, Coogan plays Octavius, a miniature Roman general. Although it is, quite literally, a small part, Coogan makes the most of sharing the screen with Robin Williams, Ben Stiller and Owen Wilson – “We may be small, but our hearts are large … metaphorically speaking!” he bellows at Stiller’s security guard. As Octavius, with Owen Wilson’s Jedediah, in Night at the Museum: Battle of the Smithsonian. Photograph: 20th Century Fox/Allstar Sold for $1'\n",
            "\n",
            "🔍 Triple: Coogan — plays — Octavius\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/California_Child_Actor%27s_Bill', 1.0), ('http://dbpedia.org/resource/Michael_Coogan', 0.5)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Augustus', 1.0), ('http://dbpedia.org/resource/Octavius_%28horse%29', 0.43243243243243246), ('http://dbpedia.org/resource/Octavius_%28praenomen%29', 0.35135135135135137), ('http://dbpedia.org/resource/Octavius_%28dialogue%29', 0.35135135135135137), ('http://dbpedia.org/resource/Prince_Octavius_of_Great_Britain', 0.24324324324324326)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/plays', 0.7926700711250305), ('http://dbpedia.org/ontology/settingOfPlay', 0.6854267716407776), ('http://dbpedia.org/ontology/subjectOfPlay', 0.6681440472602844), ('http://dbpedia.org/ontology/characterInPlay', 0.6678355932235718), ('http://dbpedia.org/ontology/playRole', 0.6664632558822632)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.793) rank=1/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Michael_Coogan\n",
            "            http://dbpedia.org/ontology/plays\n",
            "            http://dbpedia.org/resource/Augustus\n",
            "\n",
            "🔍 Triple: Eric Idle — plays — Rat\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Eric_Idle', 1.0)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Rat', 1.0), ('http://dbpedia.org/resource/Rat_%28zodiac%29', 0.768), ('http://dbpedia.org/resource/Rat_%28film%29', 0.168), ('http://dbpedia.org/resource/Rat_%28newspaper%29', 0.088), ('http://dbpedia.org/resource/Pearls_Before_Swine_%28comics%29', 0.032)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/plays', 0.7926700711250305), ('http://dbpedia.org/ontology/settingOfPlay', 0.6854267716407776), ('http://dbpedia.org/ontology/subjectOfPlay', 0.6681440472602844), ('http://dbpedia.org/ontology/characterInPlay', 0.6678355932235718), ('http://dbpedia.org/ontology/playRole', 0.6664632558822632)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.793) rank=1/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Eric_Idle\n",
            "            http://dbpedia.org/ontology/plays\n",
            "            http://dbpedia.org/resource/Rat\n",
            "\n",
            "🔍 Triple: Terry Jones — plays — Toad\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Terry_Jones', 1.0), ('http://dbpedia.org/resource/Terry_Jones_%28pastor%29', 0.06720977596741344), ('http://dbpedia.org/resource/Terry_Jones_%28racing_driver%29', 0.04684317718940937), ('http://dbpedia.org/resource/Terry_Jones_%28baseball%29', 0.034623217922606926), ('http://dbpedia.org/resource/Terry_Jones_%28i-D%29', 0.020366598778004074)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Toad_%28Marvel_Comics%29', 1.0), ('http://dbpedia.org/resource/Toad_%28Nintendo%29', 0.7205882352941176), ('http://dbpedia.org/resource/Toad', 0.3382352941176471), ('http://dbpedia.org/resource/Toad_%28instrumental%29', 0.11764705882352941), ('http://dbpedia.org/resource/Toad_%28band%29', 0.08088235294117647)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/plays', 0.7926700711250305), ('http://dbpedia.org/ontology/settingOfPlay', 0.6854267716407776), ('http://dbpedia.org/ontology/subjectOfPlay', 0.6681440472602844), ('http://dbpedia.org/ontology/characterInPlay', 0.6678355932235718), ('http://dbpedia.org/ontology/playRole', 0.6664632558822632)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.793) rank=1/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Terry_Jones\n",
            "            http://dbpedia.org/ontology/plays\n",
            "            http://dbpedia.org/resource/Toad_%28Marvel_Comics%29\n",
            "\n",
            "====================================================================================================\n",
            "[3/10] Art | You Can Now See the Guillotine Linked to Marie Antoinette’s Demise\n",
            "URL: https://news.artnet.com/art-world/marie-antoinette-execution-guillotine-va-exhibition-2699550\n",
            "Published: 2025-10-16T16:01:25+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 \"Objects relating to the legendary death of France's last Queen have gone on view at the V&A in London. The post You Can Now See the Guillotine Linked to Marie Antoinette’s Demise appeared first on Artnet News .\"\n",
            "\n",
            "🔍 Triple: V&A — in — London\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Victoria_and_Albert_Museum', 1.0)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/London', 1.0), ('http://dbpedia.org/resource/London%2C_Ontario', 0.015061644326571573)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/spokenIn', 0.7298456430435181), ('http://dbpedia.org/ontology/inflow', 0.7243426442146301), ('http://dbpedia.org/ontology/veneratedIn', 0.7171459197998047), ('http://dbpedia.org/ontology/inn', 0.7171381711959839), ('http://dbpedia.org/ontology/orderInOffice', 0.69605553150177)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.692) rank=6/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Victoria_and_Albert_Museum\n",
            "            http://dbpedia.org/ontology/recordedIn\n",
            "            http://dbpedia.org/resource/London\n",
            "\n",
            "🔍 Triple: Guillotine — have gone on view at — V&A\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Guillotine', 1.0), ('http://dbpedia.org/resource/Guillotine_%28British_India_album%29', 0.15384615384615385), ('http://dbpedia.org/resource/Guillotine_%28film%29', 0.12307692307692308), ('http://dbpedia.org/resource/List_of_Marvel_Comics_characters%3A_G', 0.1076923076923077), ('http://dbpedia.org/resource/Guillotine_%28game%29', 0.09230769230769231)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Victoria_and_Albert_Museum', 1.0)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/exhibition', 0.6514056324958801), ('http://dbpedia.org/ontology/discontinued', 0.6162174940109253), ('http://dbpedia.org/ontology/otherAppearances', 0.6084439754486084), ('http://dbpedia.org/ontology/endangeredSince', 0.6062077283859253), ('http://dbpedia.org/ontology/show', 0.6053003072738647)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.651) rank=1/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Guillotine\n",
            "            http://dbpedia.org/ontology/exhibition\n",
            "            http://dbpedia.org/resource/Victoria_and_Albert_Museum\n",
            "\n",
            "====================================================================================================\n",
            "[4/10] Art | Swifties Are Flocking to this German Museum\n",
            "URL: https://hyperallergic.com/1049620/swifties-are-flocking-to-this-german-museum/\n",
            "Published: 2025-10-15T21:35:57+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'A museum in Germany is reportedly experiencing a surge in visitors after Taylor Swift released a music video that apparently references a painting in its collection. Monopol reported that visitors flocked to the Museum Wiesbaden over the weekend to view Friedrich Wilhelm Theodor Heyser’s “Ophelia” (1900), which portrays the titular subject in a supine position, wearing a white, flowing dress, presumably drowning herself in a stream, as in the plot of the Shakespearean tragedy. A spokesperson for Museum Weisbaden told Hyperallergic that 500 guests visited the museum just to see the painting over the weekend, an increase they attributed to the Taylor Swift video release. The German artist’s rendition of Ophelia is eerily similar to one of the first scenes of Swift’s music video for “The Fate of Ophelia,” the lead track of her newest album, The Life of a Showgirl . The singer appears in a f'\n",
            "   ⚠ No triples extracted.\n",
            "\n",
            "====================================================================================================\n",
            "[5/10] Science & technology | SpaceX has plans to launch Falcon Heavy from California-if anyone wants it to\n",
            "URL: https://arstechnica.com/space/2025/10/spacex-has-plans-to-launch-falcon-heavy-from-california-if-anyone-wants-it-to/\n",
            "Published: 2025-10-16T15:40:14+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'There\\'s more to the changes at Vandenberg than launching additional rockets. The authorization gives SpaceX the green light to redevelop Space Launch Complex 6 (SLC-6) to support Falcon 9 and Falcon Heavy missions. SpaceX plans to demolish unneeded structures at SLC-6 (pronounced \"Slick 6\") and construct two new landing pads for Falcon boosters on a bluff overlooking the Pacific just south of the pad. SpaceX currently operates from a single pad at Vandenberg —Space Launch Complex 4-East (SLC-4E) —a few miles north of the SLC-6 location. The SLC-4E location is not configured to launch the Falcon Heavy, an uprated rocket with three Falcon 9 boosters bolted together. SLC-6, cocooned by hills on three sides and flanked by the ocean to the west, is no stranger to big rockets. It was first developed for the Air Force\\'s Manned Orbiting Laboratory program in the 1960s, when the military wanted t'\n",
            "\n",
            "🔍 Triple: SLC-4E — not configured to launch — Falcon Heavy\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Vandenberg_Space_Launch_Complex_4', 1.0)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Falcon_Heavy', 1.0)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/launch', 0.7049106359481812), ('http://dbpedia.org/ontology/failedLaunches', 0.6997060179710388), ('http://dbpedia.org/ontology/launches', 0.691193699836731), ('http://dbpedia.org/ontology/configuration', 0.6897604465484619), ('http://dbpedia.org/ontology/launchSite', 0.6763676404953003)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.690) rank=4/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Vandenberg_Space_Launch_Complex_4\n",
            "            http://dbpedia.org/ontology/configuration\n",
            "            http://dbpedia.org/resource/Falcon_Heavy\n",
            "\n",
            "====================================================================================================\n",
            "[6/10] Geography | Wallender Born 'Brubbel' in Wallenborn, Germany\n",
            "URL: https://www.atlasobscura.com/places/brubbel\n",
            "Published: 2025-10-16T16:00:00+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'Known locally as the Brubbel, the Wallender Born is one of the few cold-water geysers in Europe. Every 30–35 minutes, the earth seems to come alive as a fountain of water bursts upward, reaching heights of up to four meters. Unlike hot-water geysers powered by volcanic heat, the Brubbel’s eruptions are caused by carbon dioxide gas rising through underground springs, a rare natural spectacle. Information boards around the site explain the unusual geology behind the eruptions, as well as the cultural history of the Brubbel. There is a car park nearby, but the site can also be walked or cycled to.'\n",
            "   ⚠ No triples extracted.\n",
            "\n",
            "====================================================================================================\n",
            "[7/10] Science & technology | Open source GZDoom community splinters after creator inserts AI-generated code\n",
            "URL: https://arstechnica.com/gaming/2025/10/civil-war-gzdoom-fan-developers-split-off-over-use-of-chatgpt-generated-code/\n",
            "Published: 2025-10-16T16:48:00+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'That comment led to a lengthy discussion among developers about the use of \"stolen scraped code that we have no way of verifying is compatible with the GPL,\" as one described it. And while Zahl eventually removed the offending code, he also allegedly tried to remove the evidence that it ever existed by force-pushing an update to delete the discussion entirely. // This is what ChatGPT told me for detecting dark mode on Linux. Graf Zahl code comment Zahl defended the use of AI-generated snippets for \"boilerplate code\" that isn\\'t key to underlying game features. \"I surely have my reservations about using AI for project specific code,\" he wrote, \"but this here is just superficial checks of system configuration settings that can be found on various websites—just with 10x the effort required.\" But others in the community were adamant that there\\'s no place for AI tools in the workflow of an ope'\n",
            "   ⚠ No triples extracted.\n",
            "\n",
            "====================================================================================================\n",
            "[8/10] Science & technology | OnePlus’ OxygenOS 16 brings Gemini into your Mind Space\n",
            "URL: https://www.theverge.com/news/800754/oneplus-oxygenos-16-gemini-mind-space\n",
            "Published: nan\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'OnePlus has announced OxygenOS 16, its take on Android 16, with upgrades to its Mind Space AI tool including complete integration with Google Gemini. Other updates include new lock screen customizations, improved connectivity options, and design tweaks throughout. Mind Space is where the biggest changes have been made. When it launched in June , Mind Space only allowed you to save screenshots, which the AI would then analyze to extract information or create calendar entries. Now you can also save longer, scrolling screenshots to the AI locker, and record 60-second voice memos to add more information for the AI to work with. More importantly, OnePlus has worked with Google to integrate its Gemini AI Assistant, which is capable of handling tasks based on the information in your Mind Space. Save lots of screenshots of hotels, flight options, and attractions for a vacation, for example, and '\n",
            "\n",
            "🔍 Triple: OnePlus — has worked with — Google\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/OnePlus', 1.0)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Google', 1.0), ('http://dbpedia.org/resource/Google_Search', 0.019180241723594324)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/previousWork', 0.6640526652336121), ('http://dbpedia.org/ontology/hasJunctionWith', 0.6428150534629822), ('http://dbpedia.org/ontology/formerPartner', 0.6310434341430664), ('http://dbpedia.org/ontology/otherWorks', 0.6274648904800415), ('http://dbpedia.org/ontology/currentPartner', 0.6263813972473145)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.626) rank=5/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/OnePlus\n",
            "            http://dbpedia.org/ontology/currentPartner\n",
            "            http://dbpedia.org/resource/Google\n",
            "\n",
            "====================================================================================================\n",
            "[9/10] TV shows & movies | ‘She does terrible things’: what can a Marvel director do with Ibsen’s ruthless heroine Hedda Gabler?\n",
            "URL: https://www.theguardian.com/film/2025/oct/15/marvel-ibsens-hedda-gabler-nia-dacosta-tessa-thompson\n",
            "Published: 2025-10-15T16:19:26+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 'N ia DaCosta and Tessa Thompson are reminiscing about the first time they met, at Sundance film labs where DaCosta was workshopping her debut feature, Little Woods. “Honestly, Tessa had a great vibe,” says DaCosta. “She was super open, super generous and very intelligent.” A smile creeps on to her face. “Like – that was a fucking relief.” Thompson gives a look of mock offence. “I really just like working with smart actors,” adds DaCosta, filling the silence. “Why did you assume that we’re dumdums?” asks Thompson, turning to look directly at her director, as they sit in a Soho hotel in London. “I didn’t,” she is told. “I was just like, ‘What a pleasant surprise.’ Who would have thought it? Not me.” Hedda gives drinks to a recovering alcoholic – and a loaded pistol to an unstable former lover This is a typical exchange from a director and actor partnership that’s now on its third film, and'\n",
            "\n",
            "🔍 Triple: DaCosta — was workshopping — Little Woods\n",
            "   📍 Getting entity candidates from Redis...\n",
            "   [Redis:subject] [('http://dbpedia.org/resource/Amanda_DaCosta', 1.0), ('http://dbpedia.org/resource/Hammonton%2C_New_Jersey', 0.14285714285714285), ('http://dbpedia.org/resource/DaCosta', 0.14285714285714285)]\n",
            "   [Redis:object] [('http://dbpedia.org/resource/Little_Woods', 1.0), ('http://dbpedia.org/resource/Little_Woods%2C_Louisiana', 0.3684210526315789)]\n",
            "   [Predicates:top5] [('http://dbpedia.org/ontology/whaDraft', 0.6497756242752075), ('http://dbpedia.org/ontology/draft', 0.6194723844528198), ('http://dbpedia.org/ontology/previousWork', 0.6185888648033142), ('http://dbpedia.org/ontology/whaDraftTeam', 0.6175045967102051), ('http://dbpedia.org/ontology/work', 0.6108878254890442)]\n",
            "   ✅ Final\n",
            "            [CANDIDATE] (sim=0.611) rank=5/10 | thr=0.50:\n",
            "            http://dbpedia.org/resource/Amanda_DaCosta\n",
            "            http://dbpedia.org/ontology/work\n",
            "            http://dbpedia.org/resource/Little_Woods\n",
            "\n",
            "====================================================================================================\n",
            "[10/10] Geography | The Sullivan Center in Chicago, Illinois\n",
            "URL: https://www.atlasobscura.com/places/the-sullivan-center\n",
            "Published: 2025-10-16T15:00:00+00:00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📝 \"The Sullivan Center building seamlessly blends architectural history, unique design, and Bullseye-branded shopping. Once home to the Schlesinger & Mayer Department Store, the cast-iron-fronted building is now best known for its uniquely designed Target. Ornate cast iron wraps around the shop's entrance with dramatic dark colors and details that are often described as goth. Flowing shapes and twisting metal details frame the doors and windows, opening onto a wooden entryway that’s equally striking. The result is a stark and even humorous contrast to Target’s bright ads and products located within this historic building. Technically, the building’s architectural style isn’t Gothic at all. It reflects its namesake designer's love of organic beauty. Louis Sullivan drew inspiration from local Midwestern prairie plants, creating patterns of movement and natural splendor when he was commissione\"\n",
            "   ⚠ No triples extracted.\n",
            "\n",
            "✅ Saved 8 resolved triples → triple_results_recent.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  Back of the net! Steve Coogan film roles - ranked   \n",
              "1  Back of the net! Steve Coogan film roles - ranked   \n",
              "2  Back of the net! Steve Coogan film roles - ranked   \n",
              "3  You Can Now See the Guillotine Linked to Marie...   \n",
              "4  You Can Now See the Guillotine Linked to Marie...   \n",
              "5  SpaceX has plans to launch Falcon Heavy from C...   \n",
              "6  OnePlus’ OxygenOS 16 brings Gemini into your M...   \n",
              "7  ‘She does terrible things’: what can a Marvel ...   \n",
              "\n",
              "                                predicate_uri  \\\n",
              "0           http://dbpedia.org/ontology/plays   \n",
              "1           http://dbpedia.org/ontology/plays   \n",
              "2           http://dbpedia.org/ontology/plays   \n",
              "3      http://dbpedia.org/ontology/recordedIn   \n",
              "4      http://dbpedia.org/ontology/exhibition   \n",
              "5   http://dbpedia.org/ontology/configuration   \n",
              "6  http://dbpedia.org/ontology/currentPartner   \n",
              "7            http://dbpedia.org/ontology/work   \n",
              "\n",
              "                                         subject_uri  \\\n",
              "0         http://dbpedia.org/resource/Michael_Coogan   \n",
              "1              http://dbpedia.org/resource/Eric_Idle   \n",
              "2            http://dbpedia.org/resource/Terry_Jones   \n",
              "3  http://dbpedia.org/resource/Victoria_and_Alber...   \n",
              "4             http://dbpedia.org/resource/Guillotine   \n",
              "5  http://dbpedia.org/resource/Vandenberg_Space_L...   \n",
              "6                http://dbpedia.org/resource/OnePlus   \n",
              "7         http://dbpedia.org/resource/Amanda_DaCosta   \n",
              "\n",
              "                                          object_uri  \n",
              "0               http://dbpedia.org/resource/Augustus  \n",
              "1                    http://dbpedia.org/resource/Rat  \n",
              "2  http://dbpedia.org/resource/Toad_%28Marvel_Com...  \n",
              "3                 http://dbpedia.org/resource/London  \n",
              "4  http://dbpedia.org/resource/Victoria_and_Alber...  \n",
              "5           http://dbpedia.org/resource/Falcon_Heavy  \n",
              "6                 http://dbpedia.org/resource/Google  \n",
              "7           http://dbpedia.org/resource/Little_Woods  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5914936b-ef65-4406-96af-bbbb879fa457\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>predicate_uri</th>\n",
              "      <th>subject_uri</th>\n",
              "      <th>object_uri</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Back of the net! Steve Coogan film roles - ranked</td>\n",
              "      <td>http://dbpedia.org/ontology/plays</td>\n",
              "      <td>http://dbpedia.org/resource/Michael_Coogan</td>\n",
              "      <td>http://dbpedia.org/resource/Augustus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Back of the net! Steve Coogan film roles - ranked</td>\n",
              "      <td>http://dbpedia.org/ontology/plays</td>\n",
              "      <td>http://dbpedia.org/resource/Eric_Idle</td>\n",
              "      <td>http://dbpedia.org/resource/Rat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Back of the net! Steve Coogan film roles - ranked</td>\n",
              "      <td>http://dbpedia.org/ontology/plays</td>\n",
              "      <td>http://dbpedia.org/resource/Terry_Jones</td>\n",
              "      <td>http://dbpedia.org/resource/Toad_%28Marvel_Com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You Can Now See the Guillotine Linked to Marie...</td>\n",
              "      <td>http://dbpedia.org/ontology/recordedIn</td>\n",
              "      <td>http://dbpedia.org/resource/Victoria_and_Alber...</td>\n",
              "      <td>http://dbpedia.org/resource/London</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You Can Now See the Guillotine Linked to Marie...</td>\n",
              "      <td>http://dbpedia.org/ontology/exhibition</td>\n",
              "      <td>http://dbpedia.org/resource/Guillotine</td>\n",
              "      <td>http://dbpedia.org/resource/Victoria_and_Alber...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SpaceX has plans to launch Falcon Heavy from C...</td>\n",
              "      <td>http://dbpedia.org/ontology/configuration</td>\n",
              "      <td>http://dbpedia.org/resource/Vandenberg_Space_L...</td>\n",
              "      <td>http://dbpedia.org/resource/Falcon_Heavy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>OnePlus’ OxygenOS 16 brings Gemini into your M...</td>\n",
              "      <td>http://dbpedia.org/ontology/currentPartner</td>\n",
              "      <td>http://dbpedia.org/resource/OnePlus</td>\n",
              "      <td>http://dbpedia.org/resource/Google</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>‘She does terrible things’: what can a Marvel ...</td>\n",
              "      <td>http://dbpedia.org/ontology/work</td>\n",
              "      <td>http://dbpedia.org/resource/Amanda_DaCosta</td>\n",
              "      <td>http://dbpedia.org/resource/Little_Woods</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5914936b-ef65-4406-96af-bbbb879fa457')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5914936b-ef65-4406-96af-bbbb879fa457 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5914936b-ef65-4406-96af-bbbb879fa457');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ae5823d8-84ce-4dfc-aab9-9bb38219d782\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae5823d8-84ce-4dfc-aab9-9bb38219d782')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ae5823d8-84ce-4dfc-aab9-9bb38219d782 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"You Can Now See the Guillotine Linked to Marie Antoinette\\u2019s Demise\",\n          \"\\u2018She does terrible things\\u2019: what can a Marvel director do with Ibsen\\u2019s ruthless heroine Hedda Gabler?\",\n          \"SpaceX has plans to launch Falcon Heavy from California-if anyone wants it to\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicate_uri\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"http://dbpedia.org/ontology/plays\",\n          \"http://dbpedia.org/ontology/recordedIn\",\n          \"http://dbpedia.org/ontology/work\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject_uri\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"http://dbpedia.org/resource/Eric_Idle\",\n          \"http://dbpedia.org/resource/Vandenberg_Space_Launch_Complex_4\",\n          \"http://dbpedia.org/resource/Michael_Coogan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"object_uri\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"http://dbpedia.org/resource/Rat\",\n          \"http://dbpedia.org/resource/Falcon_Heavy\",\n          \"http://dbpedia.org/resource/Augustus\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#only take from redis, don't generate stuff from reids, ai startup\n",
        "\n",
        "#generalize the term, find a class\n",
        "# RDF predicate vs property\n",
        "#predicate is when you create a property\n",
        "#new propety when preidcate is not found and we need a new one\n",
        "\n",
        "#benchmark - speed, accurary"
      ],
      "metadata": {
        "id": "PhFyTFBhxOT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTe1P0Z9yVsn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}